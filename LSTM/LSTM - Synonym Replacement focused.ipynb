{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM - Synonym Replacement focused.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Pj46xqY_kNy5"},"source":["# Lemmatized, Back-translated Augmented Train"]},{"cell_type":"markdown","metadata":{"id":"2TWPQuT1JM2p"},"source":["# **This section shows the results for the back-translation dataset trained using default embeddings (for your reference to compare with synonym replacement results in this notebook).**"]},{"cell_type":"code","metadata":{"id":"6CrE_e_KvIah"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jrqG20F2Jh2"},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras import layers\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import gensim\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import matthews_corrcoef\n","\n","\n","train_df_bt = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/lemmatized_bt_augment_train.csv\")\n","test_df_bt = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/test.csv\")\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/validation.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axivysukzKoo"},"source":["def convert_prediction_1D(predictions):\n","  result = []\n","  for array in predictions:\n","    highest_proba = max(array)\n","\n","    if list(array).index(highest_proba) == 0:\n","      result.append(-1)\n","    elif list(array).index(highest_proba) == 1:\n","      result.append(0)\n","    else:\n","      result.append(1)\n","\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"Wn_R_zoOtCbt","outputId":"305516dc-e8c0-4308-d1b3-a0b94c5e03a6"},"source":["df_bt.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>lemmatized and stopwords_removed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>should uber use driverless cars to ease safety...</td>\n","      <td>0</td>\n","      <td>uber use driverless car ease safety concern</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>oh hai minorityreport is making your driverles...</td>\n","      <td>0</td>\n","      <td>oh hai minorityreport driverless transportatio...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>who is responsible if a self driving car gets ...</td>\n","      <td>0</td>\n","      <td>responsible self drive car accident</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>i almost got rear ended by the google car iron...</td>\n","      <td>-1</td>\n","      <td>got rear end google car ironic</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>self driving cars will be a hit until the firs...</td>\n","      <td>-1</td>\n","      <td>self drive car hit family hit algorithm sue</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                   lemmatized and stopwords_removed\n","0           0  ...        uber use driverless car ease safety concern\n","1           1  ...  oh hai minorityreport driverless transportatio...\n","2           2  ...                responsible self drive car accident\n","3           3  ...                     got rear end google car ironic\n","4           4  ...        self drive car hit family hit algorithm sue\n","\n","[5 rows x 4 columns]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"KAaeWHOv25Ag"},"source":["train_df_bt = train_df_bt[['lemmatized and stopwords_removed', 'sentiment']]\n","train_df_bt = train_df_bt.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","train_df_bt = train_df_bt.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","test_df_bt = test_df_bt[['lemmatized and stopwords_removed', 'sentiment']]\n","test_df_bt = test_df_bt.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","test_df_bt = test_df_bt.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","valid_df = valid_df[['lemmatized and stopwords_removed', 'sentiment']]\n","valid_df = valid_df.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","valid_df = valid_df.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-YYLJtcqwrW8"},"source":["train_df_bt['text'] = train_df_bt['text'].astype(\"str\")\n","train_df_bt['sentiment'] = train_df_bt['sentiment'].astype(\"str\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"heVoWLjd_5qK","outputId":"32877fa2-1c59-4794-fca4-eb8dbc181ac4"},"source":["# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 6000\n","\n","# Max number of words in each thread. For this one, we just used the average length of word of all sentences in the data,\n","#  to prevent overfitting to the longest sentence, as most of the words are around that length. This particular var, affects the training process ALOT.\n","\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in train_df_bt['text']])\n","\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","\n","# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df_bt['text'].values)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8601 unique tokens.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aha6vU6WAB8C","outputId":"e1c52939-dcd1-4dff-baca-489db41d26ea"},"source":["X_train = tokenizer.texts_to_sequences(train_df_bt['text'].values)\n","X_test = tokenizer.texts_to_sequences(test_df_bt['text'].values)\n","X_valid = tokenizer.texts_to_sequences(valid_df['text'].values)\n","\n","\n","X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n","X_valid = pad_sequences(X_valid, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Shape of data tensor (X_train):', X_train.shape)\n","print('Shape of data tensor (X_test):', X_test.shape)\n","print('Shape of data tensor (X_valid):', X_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor (X_train): (8162, 117)\n","Shape of data tensor (X_test): (671, 117)\n","Shape of data tensor (X_valid): (670, 117)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sF-8ly7AJb4","outputId":"b3066ed8-ea64-4ddf-9560-e6331bf0579c"},"source":["Y_train = pd.get_dummies(train_df_bt['sentiment']).values\n","Y_test = pd.get_dummies(test_df_bt['sentiment']).values\n","Y_valid = pd.get_dummies(valid_df['sentiment']).values\n","\n","\n","print('Shape of label tensor (Y_train):', Y_train.shape)\n","print('Shape of label tensor (Y_test):', Y_test.shape)\n","\n","# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_valid.shape, Y_valid.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of label tensor (Y_train): (8162, 3)\n","Shape of label tensor (Y_test): (671, 3)\n","(8162, 117) (8162, 3)\n","(670, 117) (670, 3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SArvBuMAQe2","outputId":"a8ba81f4-9916-4bc2-a2a2-63b704cf6079"},"source":["# LSTM\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(layers.Embedding(len(word_index)+1, EMBEDDING_DIM, input_length=X_train.shape[1]))\n","model.add(LSTM(128, return_sequences=True, dropout=0.5, recurrent_dropout=0.2))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","\n","model.summary()\n","\n","epochs = 25\n","batch_size = 16\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","# history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])\n","# history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), epochs=epochs, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_7 (Embedding)      (None, 117, 100)          860200    \n","_________________________________________________________________\n","lstm_14 (LSTM)               (None, 117, 128)          117248    \n","_________________________________________________________________\n","lstm_15 (LSTM)               (None, 32)                20608     \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 32)                1056      \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 999,211\n","Trainable params: 999,211\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 483s 936ms/step - loss: 0.9294 - accuracy: 0.5518 - val_loss: 0.7917 - val_accuracy: 0.6985\n","Epoch 2/25\n","511/511 [==============================] - 477s 934ms/step - loss: 0.5962 - accuracy: 0.7555 - val_loss: 0.7998 - val_accuracy: 0.6985\n","Epoch 3/25\n","511/511 [==============================] - 474s 928ms/step - loss: 0.4197 - accuracy: 0.8416 - val_loss: 0.8650 - val_accuracy: 0.6836\n","Epoch 4/25\n","511/511 [==============================] - 470s 919ms/step - loss: 0.3114 - accuracy: 0.8819 - val_loss: 0.9368 - val_accuracy: 0.6806\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7T8J2iajdie","outputId":"8ec2f3b3-6568-4fe1-fa39-7e4b1560f2c8"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 1s 56ms/step - loss: 0.9848 - accuracy: 0.6632\n","Test set\n"," Loss: 0.985\n","  Test Accuracy: 0.663\n","Precision is:  0.6660694240451904\n","Recall is:  0.6631892697466468\n","F1 Score is:  0.6602634488711754\n","Matthew Corr Score is:  0.3738437698084896\n"]}]},{"cell_type":"markdown","metadata":{"id":"d9D_YgWSS9CQ"},"source":["# Lemmatized, Original Data"]},{"cell_type":"markdown","metadata":{"id":"Jca9tx0cJiZe"},"source":["# **This section shows the results for the original dataset trained using default embeddings (for your reference to compare with synonym replacement results).**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDw3fkpjS_uA","outputId":"2d365a10-e82e-42cd-d66a-29ef5f84ec39"},"source":["train_df_lem = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/lemmatized_original_train.csv\")\n","test_df_lem = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/test.csv\")\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/validation.csv\")\n","\n","train_df_lem = train_df_lem[['lemmatized and stopwords_removed', 'sentiment']]\n","train_df_lem = train_df_lem.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","train_df_lem = train_df_bt.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","test_df_lem = test_df_lem[['lemmatized and stopwords_removed', 'sentiment']]\n","test_df_lem = test_df_lem.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","test_df_lem = test_df_lem.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","valid_df = valid_df[['lemmatized and stopwords_removed', 'sentiment']]\n","valid_df = valid_df.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","valid_df = valid_df.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","train_df_lem['text'] = train_df_lem['text'].astype(\"str\")\n","train_df_lem['sentiment'] = train_df_lem['sentiment'].astype(\"str\")\n","\n","# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 6000\n","\n","# Max number of words in each thread. For this one, we just used the average length of word of all sentences in the data,\n","#  to prevent overfitting to the longest sentence, as most of the words are around that length. This particular var, affects the training process ALOT.\n","\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in train_df_lem['text']])\n","\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","\n","# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df_lem['text'].values)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","X_train = tokenizer.texts_to_sequences(train_df_lem['text'].values)\n","X_test = tokenizer.texts_to_sequences(test_df_lem['text'].values)\n","X_valid = tokenizer.texts_to_sequences(valid_df['text'].values)\n","\n","\n","X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n","X_valid = pad_sequences(X_valid, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Shape of data tensor (X_train):', X_train.shape)\n","print('Shape of data tensor (X_test):', X_test.shape)\n","print('Shape of data tensor (X_valid):', X_valid.shape)\n","\n","Y_train = pd.get_dummies(train_df_lem['sentiment']).values\n","Y_test = pd.get_dummies(test_df_lem['sentiment']).values\n","Y_valid = pd.get_dummies(valid_df['sentiment']).values\n","\n","print('Shape of label tensor (Y_train):', Y_train.shape)\n","print('Shape of label tensor (Y_test):', Y_test.shape)\n","\n","# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_valid.shape, Y_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8601 unique tokens.\n","Shape of data tensor (X_train): (8162, 117)\n","Shape of data tensor (X_test): (671, 117)\n","Shape of data tensor (X_valid): (670, 117)\n","Shape of label tensor (Y_train): (8162, 3)\n","Shape of label tensor (Y_test): (671, 3)\n","(8162, 117) (8162, 3)\n","(670, 117) (670, 3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3YRnDIpTu0U","outputId":"e5ba7430-b2de-4af8-ad9f-43f63504eb12"},"source":["# LSTM\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(layers.Embedding(len(word_index)+1, EMBEDDING_DIM, input_length=X_train.shape[1]))\n","model.add(LSTM(128, return_sequences=True, dropout=0.5, recurrent_dropout=0.2))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","\n","model.summary()\n","\n","epochs = 25\n","batch_size = 16\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, 117, 100)          860200    \n","_________________________________________________________________\n","lstm_12 (LSTM)               (None, 117, 128)          117248    \n","_________________________________________________________________\n","lstm_13 (LSTM)               (None, 32)                20608     \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 32)                1056      \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 999,211\n","Trainable params: 999,211\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 486s 942ms/step - loss: 0.9355 - accuracy: 0.5368 - val_loss: 0.7367 - val_accuracy: 0.6910\n","Epoch 2/25\n","511/511 [==============================] - 473s 926ms/step - loss: 0.5867 - accuracy: 0.7627 - val_loss: 0.8125 - val_accuracy: 0.7045\n","Epoch 3/25\n","511/511 [==============================] - 477s 933ms/step - loss: 0.3948 - accuracy: 0.8503 - val_loss: 0.8567 - val_accuracy: 0.6776\n","Epoch 4/25\n","511/511 [==============================] - 475s 930ms/step - loss: 0.2860 - accuracy: 0.8965 - val_loss: 0.9913 - val_accuracy: 0.6687\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INFQdiJLXV8U","outputId":"bacd108b-aa83-41ce-db1a-f3b84dd5c221"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 2s 74ms/step - loss: 1.0861 - accuracy: 0.6170\n","Test set\n"," Loss: 1.086\n","  Test Accuracy: 0.617\n","Precision is:  0.6032164042042557\n","Recall is:  0.61698956780924\n","F1 Score is:  0.6069937080316189\n","Matthew Corr Score is:  0.26197648657716915\n"]}]},{"cell_type":"markdown","metadata":{"id":"IfKwKr3CXdRe"},"source":["# Lemmatized, Synonym, Augmented Data\n"]},{"cell_type":"markdown","metadata":{"id":"6tMGMK9UJuLD"},"source":["# **This section shows the results for the synonym replacement dataset trained using default embeddings.**"]},{"cell_type":"markdown","metadata":{"id":"NTvhTjYq_5Py"},"source":["***Modelling with synonym replacement dataset***\n","\n","<ul>\n","  <li> LSTM with default Embeddings (results are in the section above) </li>\n","  <li> LSTM with Word2Vec Embeddings </li>\n","  <li> LSTM with Pre-trained Word2Vec Embeddings </li>\n","  <li> LSTM with Glove Embeddings </li>\n","</ul>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDYnUbuOXhUY","outputId":"3962e65b-a585-4f8c-853f-a83b53c65e33"},"source":["train_df_syn = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/lemmatized_synonym_augment_train.csv\")\n","test_df_lem = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/test.csv\")\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/validation.csv\")\n","\n","train_df_syn = train_df_syn[['lemmatized and stopwords_removed', 'sentiment']]\n","train_df_syn = train_df_syn.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","train_df_syn = train_df_syn.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","test_df_lem = test_df_lem[['lemmatized and stopwords_removed', 'sentiment']]\n","test_df_lem = test_df_lem.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","test_df_lem = test_df_lem.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","valid_df = valid_df[['lemmatized and stopwords_removed', 'sentiment']]\n","valid_df = valid_df.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","valid_df = valid_df.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","train_df_syn['text'] = train_df_syn['text'].astype(\"str\")\n","train_df_syn['sentiment'] = train_df_syn['sentiment'].astype(\"str\")\n","\n","# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 6000\n","\n","# Max number of words in each thread. For this one, we just used the average length of word of all sentences in the data,\n","#  to prevent overfitting to the longest sentence, as most of the words are around that length. This particular var, affects the training process ALOT.\n","\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in train_df_syn['text']])\n","\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","\n","# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df_syn['text'].values)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","X_train = tokenizer.texts_to_sequences(train_df_syn['text'].values)\n","X_test = tokenizer.texts_to_sequences(test_df_lem['text'].values)\n","X_valid = tokenizer.texts_to_sequences(valid_df['text'].values)\n","\n","\n","X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n","X_valid = pad_sequences(X_valid, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Shape of data tensor (X_train):', X_train.shape)\n","print('Shape of data tensor (X_test):', X_test.shape)\n","print('Shape of data tensor (X_valid):', X_valid.shape)\n","\n","Y_train = pd.get_dummies(train_df_syn['sentiment']).values\n","Y_test = pd.get_dummies(test_df_lem['sentiment']).values\n","Y_valid = pd.get_dummies(valid_df['sentiment']).values\n","\n","print('Shape of label tensor (Y_train):', Y_train.shape)\n","print('Shape of label tensor (Y_test):', Y_test.shape)\n","\n","# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_valid.shape, Y_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 7454 unique tokens.\n","Shape of data tensor (X_train): (8162, 25)\n","Shape of data tensor (X_test): (671, 25)\n","Shape of data tensor (X_valid): (670, 25)\n","Shape of label tensor (Y_train): (8162, 3)\n","Shape of label tensor (Y_test): (671, 3)\n","(8162, 25) (8162, 3)\n","(670, 25) (670, 3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYTPr88EX4Km","outputId":"e184f67a-75b8-48ff-8706-477d7c80256d"},"source":["# LSTM\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(layers.Embedding(len(word_index)+1, EMBEDDING_DIM, input_length=X_train.shape[1]))\n","model.add(LSTM(128, return_sequences=True, dropout=0.5, recurrent_dropout=0.2))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","\n","model.summary()\n","\n","epochs = 25\n","batch_size = 16\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (None, 25, 100)           745500    \n","_________________________________________________________________\n","lstm_10 (LSTM)               (None, 25, 128)           117248    \n","_________________________________________________________________\n","lstm_11 (LSTM)               (None, 32)                20608     \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 32)                1056      \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 884,511\n","Trainable params: 884,511\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 117s 219ms/step - loss: 0.9484 - accuracy: 0.5290 - val_loss: 0.8030 - val_accuracy: 0.6567\n","Epoch 2/25\n","511/511 [==============================] - 112s 219ms/step - loss: 0.5851 - accuracy: 0.7627 - val_loss: 0.8578 - val_accuracy: 0.6627\n","Epoch 3/25\n","511/511 [==============================] - 112s 220ms/step - loss: 0.3903 - accuracy: 0.8547 - val_loss: 0.9339 - val_accuracy: 0.6343\n","Epoch 4/25\n","511/511 [==============================] - 114s 224ms/step - loss: 0.2904 - accuracy: 0.8929 - val_loss: 1.0020 - val_accuracy: 0.6791\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KX6Q7ElyaG3C","outputId":"32afa2aa-5dcd-4313-f157-ae37dd0c709e"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 21ms/step - loss: 1.0193 - accuracy: 0.6662\n","Test set\n"," Loss: 1.019\n","  Test Accuracy: 0.666\n","Precision is:  0.6570211455312583\n","Recall is:  0.6661698956780924\n","F1 Score is:  0.6596717180857491\n","Matthew Corr Score is:  0.35635645703901353\n"]}]},{"cell_type":"markdown","metadata":{"id":"bZwhbf0TKVS7"},"source":["# **This section will show the results for the different embeddings, for the synonym replacement dataset.**"]},{"cell_type":"markdown","metadata":{"id":"QyJjmSS0h9Oj"},"source":["# ***CBOW + LSTM (Highest Performance from above)***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukBrDCNCDRNK","outputId":"81796efa-001c-4fb5-883e-ff744aa02441"},"source":["\"\"\"\n","CBOW Model\n","\"\"\"\n","text_sentences = train_df_syn['text'].apply(lambda x: x.split())\n","\n","model = gensim.models.Word2Vec(sentences=text_sentences, size=100, window=5, workers=4, min_count=1)\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","filename = 'selfdriving_embedding_word2vec.txt'\n","model.wv.save_word2vec_format(filename, binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7459\n"]}]},{"cell_type":"code","metadata":{"id":"un6CH7CdDClK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bff3cdf8-4ae9-40b3-9689-1888f750aefd"},"source":["import numpy as np\n","embeddings_index = {}\n","f = open(\"/content/selfdriving_embedding_word2vec.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","word2vec_embedding_matrix = np.zeros((num_words, 100))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    word2vec_embedding_matrix[i] = embedding_vector \n","print(num_words)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7455\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y51Jl4_zEGzZ","outputId":"f7a068be-a14c-4768-97c3-393f8d2bd5db"},"source":["# CBOW LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 100, weights=[word2vec_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(256, return_sequences=True, dropout=0.5))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 16\n","\n","model.summary()\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","511/511 [==============================] - 16s 24ms/step - loss: 1.0664 - accuracy: 0.4055 - val_loss: 0.9572 - val_accuracy: 0.5761\n","Epoch 2/25\n","511/511 [==============================] - 10s 20ms/step - loss: 1.0599 - accuracy: 0.4218 - val_loss: 0.9992 - val_accuracy: 0.4910\n","Epoch 3/25\n","511/511 [==============================] - 10s 20ms/step - loss: 1.0571 - accuracy: 0.4271 - val_loss: 0.9830 - val_accuracy: 0.5164\n","Epoch 4/25\n","511/511 [==============================] - 10s 20ms/step - loss: 1.0544 - accuracy: 0.4345 - val_loss: 0.9841 - val_accuracy: 0.5075\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKaspBopkWYo","outputId":"3d81d511-f4ee-4b85-944e-12406205d9ce"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 9ms/step - loss: 0.9881 - accuracy: 0.5022\n","Test set\n"," Loss: 0.988\n","  Test Accuracy: 0.502\n","Precision is:  0.504536840056234\n","Recall is:  0.5022354694485842\n","F1 Score is:  0.4939524467893189\n","Matthew Corr Score is:  0.07485013625354814\n"]}]},{"cell_type":"markdown","metadata":{"id":"0c6K-zJbh48e"},"source":["# ***Skipgram Word2Vec LSTM***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L44iW96eEG12","outputId":"54739051-d9a6-49b8-fab3-181a2cd66304"},"source":["\"\"\"\n","Skip-Gram Model\n","\"\"\"\n","\n","text_sentences = train_df_syn['text'].apply(lambda x: x.split())\n","\n","model = gensim.models.Word2Vec(sentences=text_sentences, size=100, window=5, workers=4, min_count=1, sg=1)\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","filename = 'selfdriving_embedding_word2vec_skipgram.txt'\n","model.wv.save_word2vec_format(filename, binary=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7459\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkEm-eHCEG4D","outputId":"97b21121-bb8a-44d1-a613-50f735c11727"},"source":["# SkipGram word2vec\n","import numpy as np\n","embeddings_index = {}\n","f = open(\"/content/selfdriving_embedding_word2vec_skipgram.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","word2vec_embedding_matrix = np.zeros((num_words, 100))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    word2vec_embedding_matrix[i] = embedding_vector \n","print(num_words)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7455\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SGNEvlicEG5z","outputId":"80bb2070-25b6-4701-d8c2-2168979c68c4"},"source":["# Skipgram LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 100, weights=[word2vec_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(256, return_sequences=True, dropout=0.5))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 16\n","\n","model.summary()\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","511/511 [==============================] - 15s 22ms/step - loss: 1.0666 - accuracy: 0.4173 - val_loss: 0.9444 - val_accuracy: 0.5672\n","Epoch 2/25\n","511/511 [==============================] - 10s 20ms/step - loss: 1.0572 - accuracy: 0.4331 - val_loss: 0.9708 - val_accuracy: 0.5582\n","Epoch 3/25\n","511/511 [==============================] - 10s 20ms/step - loss: 1.0535 - accuracy: 0.4369 - val_loss: 0.9661 - val_accuracy: 0.5269\n","Epoch 4/25\n","511/511 [==============================] - 10s 20ms/step - loss: 1.0420 - accuracy: 0.4467 - val_loss: 1.0141 - val_accuracy: 0.4627\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3c2tO_kXEG8V","outputId":"dce717da-c5b8-4cb0-b59f-421ee0effa15"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 9ms/step - loss: 1.0201 - accuracy: 0.4382\n","Test set\n"," Loss: 1.020\n","  Test Accuracy: 0.438\n","Precision is:  0.5293929347825737\n","Recall is:  0.43815201192250375\n","F1 Score is:  0.43405014169882217\n","Matthew Corr Score is:  0.10330919956385144\n"]}]},{"cell_type":"markdown","metadata":{"id":"qDejR6wWh1yG"},"source":["# ***Glove + LSTM***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HohnZV_AMkr1","outputId":"e365ec8b-36b0-4826-f404-75bf88ac5758"},"source":["# Glove\n","import numpy as np\n","embeddings_index = {}\n","# f = open(\"/content/depression_embedding_word2vec.txt\")\n","f = open(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/glove.6B.300d.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, 300))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector \n","print(num_words)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8602\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6JJBVP0Mkwo","outputId":"a6f593c2-2985-4027-aa4c-cf779597a6f0"},"source":["# Glove LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(256, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","model.summary()\n","\n","epochs = 25\n","batch_size = 16\n","\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","511/511 [==============================] - 16s 23ms/step - loss: 1.2953 - accuracy: 0.5159 - val_loss: 1.0175 - val_accuracy: 0.6358\n","Epoch 2/25\n","511/511 [==============================] - 11s 21ms/step - loss: 1.0449 - accuracy: 0.5633 - val_loss: 1.0111 - val_accuracy: 0.5612\n","Epoch 3/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.9684 - accuracy: 0.5863 - val_loss: 0.8712 - val_accuracy: 0.6448\n","Epoch 4/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.9295 - accuracy: 0.6081 - val_loss: 0.9293 - val_accuracy: 0.6224\n","Epoch 5/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.9220 - accuracy: 0.6121 - val_loss: 0.8360 - val_accuracy: 0.6791\n","Epoch 6/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.9044 - accuracy: 0.6274 - val_loss: 0.8543 - val_accuracy: 0.6701\n","Epoch 7/25\n","511/511 [==============================] - 11s 22ms/step - loss: 0.8892 - accuracy: 0.6433 - val_loss: 0.8343 - val_accuracy: 0.6821\n","Epoch 8/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.8725 - accuracy: 0.6506 - val_loss: 0.8613 - val_accuracy: 0.6716\n","Epoch 9/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.8689 - accuracy: 0.6636 - val_loss: 0.8873 - val_accuracy: 0.6627\n","Epoch 10/25\n","511/511 [==============================] - 11s 22ms/step - loss: 0.8648 - accuracy: 0.6659 - val_loss: 0.8602 - val_accuracy: 0.6746\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liv8G6GrEG-i","outputId":"84c5dfa4-e11b-4478-eeef-a68093376094"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 10ms/step - loss: 0.8514 - accuracy: 0.6990\n","Test set\n"," Loss: 0.851\n","  Test Accuracy: 0.699\n","Precision is:  0.6891411269375106\n","Recall is:  0.698956780923994\n","F1 Score is:  0.6881541452734682\n","Matthew Corr Score is:  0.4127671105770881\n"]}]},{"cell_type":"markdown","metadata":{"id":"cnKLeM2Ghte7"},"source":["# ***Word2Vec Pretrained w/ LSTM***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3JHIPBOe375","outputId":"06691bfd-7386-4a2f-aa4f-a0bca6d36fc5"},"source":["train_df_syn = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/lemmatized_synonym_augment_train.csv\")\n","test_df_lem = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/test.csv\")\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/validation.csv\")\n","\n","train_df_syn = train_df_syn[['lemmatized and stopwords_removed', 'sentiment']]\n","train_df_syn = train_df_syn.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","train_df_syn = train_df_syn.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","test_df_lem = test_df_lem[['lemmatized and stopwords_removed', 'sentiment']]\n","test_df_lem = test_df_lem.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","test_df_lem = test_df_lem.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","valid_df = valid_df[['lemmatized and stopwords_removed', 'sentiment']]\n","valid_df = valid_df.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","valid_df = valid_df.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","train_df_syn['text'] = train_df_syn['text'].astype(\"str\")\n","train_df_syn['sentiment'] = train_df_syn['sentiment'].astype(\"str\")\n","\n","# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 6000\n","\n","# Max number of words in each thread. For this one, we just used the average length of word of all sentences in the data,\n","#  to prevent overfitting to the longest sentence, as most of the words are around that length. This particular var, affects the training process ALOT.\n","\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in train_df_syn['text']])\n","\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","\n","# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df_syn['text'].values)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","X_train = tokenizer.texts_to_sequences(train_df_syn['text'].values)\n","X_test = tokenizer.texts_to_sequences(test_df_lem['text'].values)\n","X_valid = tokenizer.texts_to_sequences(valid_df['text'].values)\n","\n","\n","X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n","X_valid = pad_sequences(X_valid, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Shape of data tensor (X_train):', X_train.shape)\n","print('Shape of data tensor (X_test):', X_test.shape)\n","print('Shape of data tensor (X_valid):', X_valid.shape)\n","\n","Y_train = pd.get_dummies(train_df_syn['sentiment']).values\n","Y_test = pd.get_dummies(test_df_lem['sentiment']).values\n","Y_valid = pd.get_dummies(valid_df['sentiment']).values\n","\n","print('Shape of label tensor (Y_train):', Y_train.shape)\n","print('Shape of label tensor (Y_test):', Y_test.shape)\n","\n","# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_valid.shape, Y_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 7454 unique tokens.\n","Shape of data tensor (X_train): (8162, 25)\n","Shape of data tensor (X_test): (671, 25)\n","Shape of data tensor (X_valid): (670, 25)\n","Shape of label tensor (Y_train): (8162, 3)\n","Shape of label tensor (Y_test): (671, 3)\n","(8162, 25) (8162, 3)\n","(670, 25) (670, 3)\n"]}]},{"cell_type":"code","metadata":{"id":"t9OwXZbXQeAo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a52ebb3e-4f37-4a15-e511-837d81fe415e"},"source":["import gensim.downloader as api\n","model = api.load(\"word2vec-google-news-300\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCfeydCTPW_r","outputId":"6f0197ba-8f6a-48e9-a3e5-972093b35351"},"source":["\"\"\"\n","Pre-trained Word2Vec Model\n","\"\"\"\n","\n","text_sentences = df['text'].apply(lambda x: x.split())\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","filename = 'selfdriving_embedding_word2vec_pretrained.txt'\n","model.wv.save_word2vec_format(filename, binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  # Remove the CWD from sys.path while we load stuff.\n"]},{"output_type":"stream","name":"stdout","text":["3000000\n"]}]},{"cell_type":"code","metadata":{"id":"ZuawQqNGdppK"},"source":["import numpy as np\n","labels = np.asarray(model.index2word)\n","vectors = np.asarray(model.vectors)\n","word_embeddings = dict(zip(labels, vectors))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1sLEiozd03v","outputId":"dcfd3bba-58bb-46b4-c34f-274ddfd4ffec"},"source":["num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, 300))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = word_embeddings.get(word) \n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector \n","print(num_words)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7455\n"]}]},{"cell_type":"code","metadata":{"id":"NFhIqkMEPXER","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2edae1c1-3e3c-488d-f5d7-50aaab2001c8"},"source":["# Pre-trained Word2Vec LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(256, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","model.summary() \n","\n","epochs = 25\n","batch_size = 16\n","\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 25, 300)           2236500   \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 25, 256)           570368    \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 32)                36992     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 32)                1056      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 3)                 99        \n","=================================================================\n","Total params: 2,845,015\n","Trainable params: 608,515\n","Non-trainable params: 2,236,500\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 15s 23ms/step - loss: 1.2577 - accuracy: 0.4973 - val_loss: 0.9907 - val_accuracy: 0.6179\n","Epoch 2/25\n","511/511 [==============================] - 11s 21ms/step - loss: 1.0277 - accuracy: 0.5649 - val_loss: 0.9032 - val_accuracy: 0.6478\n","Epoch 3/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.9581 - accuracy: 0.5829 - val_loss: 0.8118 - val_accuracy: 0.6791\n","Epoch 4/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.9359 - accuracy: 0.5996 - val_loss: 0.7909 - val_accuracy: 0.6881\n","Epoch 5/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.9219 - accuracy: 0.6108 - val_loss: 0.7840 - val_accuracy: 0.6821\n","Epoch 6/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.9203 - accuracy: 0.6117 - val_loss: 0.7929 - val_accuracy: 0.6866\n","Epoch 7/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.8986 - accuracy: 0.6230 - val_loss: 0.8235 - val_accuracy: 0.6761\n","Epoch 8/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.8925 - accuracy: 0.6262 - val_loss: 0.7828 - val_accuracy: 0.7075\n","Epoch 9/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.8968 - accuracy: 0.6195 - val_loss: 0.8136 - val_accuracy: 0.6746\n","Epoch 10/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.8835 - accuracy: 0.6366 - val_loss: 0.8044 - val_accuracy: 0.6851\n","Epoch 11/25\n","511/511 [==============================] - 11s 21ms/step - loss: 0.8841 - accuracy: 0.6323 - val_loss: 0.8260 - val_accuracy: 0.6851\n"]}]},{"cell_type":"code","metadata":{"id":"HNMx6-OUPXGw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad00674a-fad2-48bc-c59f-f398647ee155"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 9ms/step - loss: 0.8336 - accuracy: 0.6900\n","Test set\n"," Loss: 0.834\n","  Test Accuracy: 0.690\n","Precision is:  0.6880033223014979\n","Recall is:  0.6900149031296572\n","F1 Score is:  0.6875563706040607\n","Matthew Corr Score is:  0.4137273539103003\n"]}]}]}