{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM - Back Translation focused.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Pj46xqY_kNy5"},"source":["# Lemmatized, Back-translated Augmented Train"]},{"cell_type":"markdown","metadata":{"id":"J88gFFhRJ14J"},"source":["# **This section shows the results for the back-translation dataset trained using default embeddings.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CrE_e_KvIah","outputId":"7717a841-4f75-47c0-b207-627dadceefc8"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"3jrqG20F2Jh2"},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras import layers\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import gensim\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import matthews_corrcoef\n","\n","\n","train_df_bt = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/lemmatized_bt_augment_train.csv\")\n","test_df_bt = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/test.csv\")\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/validation.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axivysukzKoo"},"source":["def convert_prediction_1D(predictions):\n","  result = []\n","  for array in predictions:\n","    highest_proba = max(array)\n","\n","    if list(array).index(highest_proba) == 0:\n","      result.append(-1)\n","    elif list(array).index(highest_proba) == 1:\n","      result.append(0)\n","    else:\n","      result.append(1)\n","\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"Wn_R_zoOtCbt","outputId":"305516dc-e8c0-4308-d1b3-a0b94c5e03a6"},"source":["df_bt.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>lemmatized and stopwords_removed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>should uber use driverless cars to ease safety...</td>\n","      <td>0</td>\n","      <td>uber use driverless car ease safety concern</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>oh hai minorityreport is making your driverles...</td>\n","      <td>0</td>\n","      <td>oh hai minorityreport driverless transportatio...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>who is responsible if a self driving car gets ...</td>\n","      <td>0</td>\n","      <td>responsible self drive car accident</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>i almost got rear ended by the google car iron...</td>\n","      <td>-1</td>\n","      <td>got rear end google car ironic</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>self driving cars will be a hit until the firs...</td>\n","      <td>-1</td>\n","      <td>self drive car hit family hit algorithm sue</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                   lemmatized and stopwords_removed\n","0           0  ...        uber use driverless car ease safety concern\n","1           1  ...  oh hai minorityreport driverless transportatio...\n","2           2  ...                responsible self drive car accident\n","3           3  ...                     got rear end google car ironic\n","4           4  ...        self drive car hit family hit algorithm sue\n","\n","[5 rows x 4 columns]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"KAaeWHOv25Ag"},"source":["train_df_bt = train_df_bt[['lemmatized and stopwords_removed', 'sentiment']]\n","train_df_bt = train_df_bt.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","train_df_bt = train_df_bt.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","test_df_bt = test_df_bt[['lemmatized and stopwords_removed', 'sentiment']]\n","test_df_bt = test_df_bt.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","test_df_bt = test_df_bt.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","valid_df = valid_df[['lemmatized and stopwords_removed', 'sentiment']]\n","valid_df = valid_df.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","valid_df = valid_df.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-YYLJtcqwrW8"},"source":["train_df_bt['text'] = train_df_bt['text'].astype(\"str\")\n","train_df_bt['sentiment'] = train_df_bt['sentiment'].astype(\"str\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"heVoWLjd_5qK","outputId":"6cee3607-a6ad-4320-b3b6-62a1d7f74613"},"source":["# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 6000\n","\n","# Max number of words in each thread. For this one, we just used the average length of word of all sentences in the data,\n","#  to prevent overfitting to the longest sentence, as most of the words are around that length. This particular var, affects the training process ALOT.\n","\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in train_df_bt['text']])\n","\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","\n","# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df_bt['text'].values)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6691 unique tokens.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aha6vU6WAB8C","outputId":"6736f73c-55aa-4240-81e6-f0e406b9c314"},"source":["X_train = tokenizer.texts_to_sequences(train_df_bt['text'].values)\n","X_test = tokenizer.texts_to_sequences(test_df_bt['text'].values)\n","X_valid = tokenizer.texts_to_sequences(valid_df['text'].values)\n","\n","\n","X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n","X_valid = pad_sequences(X_valid, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Shape of data tensor (X_train):', X_train.shape)\n","print('Shape of data tensor (X_test):', X_test.shape)\n","print('Shape of data tensor (X_valid):', X_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor (X_train): (8162, 43)\n","Shape of data tensor (X_test): (671, 43)\n","Shape of data tensor (X_valid): (670, 43)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sF-8ly7AJb4","outputId":"444c3b40-a1bd-4ee5-a36b-de89dcd55e9b"},"source":["Y_train = pd.get_dummies(train_df_bt['sentiment']).values\n","Y_test = pd.get_dummies(test_df_bt['sentiment']).values\n","Y_valid = pd.get_dummies(valid_df['sentiment']).values\n","\n","\n","print('Shape of label tensor (Y_train):', Y_train.shape)\n","print('Shape of label tensor (Y_test):', Y_test.shape)\n","\n","# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_valid.shape, Y_valid.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of label tensor (Y_train): (8162, 3)\n","Shape of label tensor (Y_test): (671, 3)\n","(8162, 43) (8162, 3)\n","(670, 43) (670, 3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SArvBuMAQe2","outputId":"a8ba81f4-9916-4bc2-a2a2-63b704cf6079"},"source":["# LSTM\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(layers.Embedding(len(word_index)+1, EMBEDDING_DIM, input_length=X_train.shape[1]))\n","model.add(LSTM(128, return_sequences=True, dropout=0.5, recurrent_dropout=0.2))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","\n","model.summary()\n","\n","epochs = 25\n","batch_size = 16\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","# history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])\n","# history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), epochs=epochs, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_7 (Embedding)      (None, 117, 100)          860200    \n","_________________________________________________________________\n","lstm_14 (LSTM)               (None, 117, 128)          117248    \n","_________________________________________________________________\n","lstm_15 (LSTM)               (None, 32)                20608     \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 32)                1056      \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 999,211\n","Trainable params: 999,211\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 483s 936ms/step - loss: 0.9294 - accuracy: 0.5518 - val_loss: 0.7917 - val_accuracy: 0.6985\n","Epoch 2/25\n","511/511 [==============================] - 477s 934ms/step - loss: 0.5962 - accuracy: 0.7555 - val_loss: 0.7998 - val_accuracy: 0.6985\n","Epoch 3/25\n","511/511 [==============================] - 474s 928ms/step - loss: 0.4197 - accuracy: 0.8416 - val_loss: 0.8650 - val_accuracy: 0.6836\n","Epoch 4/25\n","511/511 [==============================] - 470s 919ms/step - loss: 0.3114 - accuracy: 0.8819 - val_loss: 0.9368 - val_accuracy: 0.6806\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7T8J2iajdie","outputId":"8ec2f3b3-6568-4fe1-fa39-7e4b1560f2c8"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 1s 56ms/step - loss: 0.9848 - accuracy: 0.6632\n","Test set\n"," Loss: 0.985\n","  Test Accuracy: 0.663\n","Precision is:  0.6660694240451904\n","Recall is:  0.6631892697466468\n","F1 Score is:  0.6602634488711754\n","Matthew Corr Score is:  0.3738437698084896\n"]}]},{"cell_type":"markdown","metadata":{"id":"d9D_YgWSS9CQ"},"source":["# Lemmatized, Original Data"]},{"cell_type":"markdown","metadata":{"id":"M0SB9RlkJ5YT"},"source":["# **This section shows the results for the original dataset trained using default embeddings (for your reference to compare with back-translation results in this notebook).**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDw3fkpjS_uA","outputId":"2d365a10-e82e-42cd-d66a-29ef5f84ec39"},"source":["train_df_lem = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/lemmatized_original_train.csv\")\n","test_df_lem = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/test.csv\")\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/validation.csv\")\n","\n","train_df_lem = train_df_lem[['lemmatized and stopwords_removed', 'sentiment']]\n","train_df_lem = train_df_lem.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","train_df_lem = train_df_bt.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","test_df_lem = test_df_lem[['lemmatized and stopwords_removed', 'sentiment']]\n","test_df_lem = test_df_lem.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","test_df_lem = test_df_lem.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","valid_df = valid_df[['lemmatized and stopwords_removed', 'sentiment']]\n","valid_df = valid_df.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","valid_df = valid_df.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","train_df_lem['text'] = train_df_lem['text'].astype(\"str\")\n","train_df_lem['sentiment'] = train_df_lem['sentiment'].astype(\"str\")\n","\n","# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 6000\n","\n","# Max number of words in each thread. For this one, we just used the average length of word of all sentences in the data,\n","#  to prevent overfitting to the longest sentence, as most of the words are around that length. This particular var, affects the training process ALOT.\n","\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in train_df_lem['text']])\n","\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","\n","# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df_lem['text'].values)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","X_train = tokenizer.texts_to_sequences(train_df_lem['text'].values)\n","X_test = tokenizer.texts_to_sequences(test_df_lem['text'].values)\n","X_valid = tokenizer.texts_to_sequences(valid_df['text'].values)\n","\n","\n","X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n","X_valid = pad_sequences(X_valid, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Shape of data tensor (X_train):', X_train.shape)\n","print('Shape of data tensor (X_test):', X_test.shape)\n","print('Shape of data tensor (X_valid):', X_valid.shape)\n","\n","Y_train = pd.get_dummies(train_df_lem['sentiment']).values\n","Y_test = pd.get_dummies(test_df_lem['sentiment']).values\n","Y_valid = pd.get_dummies(valid_df['sentiment']).values\n","\n","print('Shape of label tensor (Y_train):', Y_train.shape)\n","print('Shape of label tensor (Y_test):', Y_test.shape)\n","\n","# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_valid.shape, Y_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8601 unique tokens.\n","Shape of data tensor (X_train): (8162, 117)\n","Shape of data tensor (X_test): (671, 117)\n","Shape of data tensor (X_valid): (670, 117)\n","Shape of label tensor (Y_train): (8162, 3)\n","Shape of label tensor (Y_test): (671, 3)\n","(8162, 117) (8162, 3)\n","(670, 117) (670, 3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3YRnDIpTu0U","outputId":"e5ba7430-b2de-4af8-ad9f-43f63504eb12"},"source":["# LSTM\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(layers.Embedding(len(word_index)+1, EMBEDDING_DIM, input_length=X_train.shape[1]))\n","model.add(LSTM(128, return_sequences=True, dropout=0.5, recurrent_dropout=0.2))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","\n","model.summary()\n","\n","epochs = 25\n","batch_size = 16\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, 117, 100)          860200    \n","_________________________________________________________________\n","lstm_12 (LSTM)               (None, 117, 128)          117248    \n","_________________________________________________________________\n","lstm_13 (LSTM)               (None, 32)                20608     \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 32)                1056      \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 999,211\n","Trainable params: 999,211\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 486s 942ms/step - loss: 0.9355 - accuracy: 0.5368 - val_loss: 0.7367 - val_accuracy: 0.6910\n","Epoch 2/25\n","511/511 [==============================] - 473s 926ms/step - loss: 0.5867 - accuracy: 0.7627 - val_loss: 0.8125 - val_accuracy: 0.7045\n","Epoch 3/25\n","511/511 [==============================] - 477s 933ms/step - loss: 0.3948 - accuracy: 0.8503 - val_loss: 0.8567 - val_accuracy: 0.6776\n","Epoch 4/25\n","511/511 [==============================] - 475s 930ms/step - loss: 0.2860 - accuracy: 0.8965 - val_loss: 0.9913 - val_accuracy: 0.6687\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INFQdiJLXV8U","outputId":"bacd108b-aa83-41ce-db1a-f3b84dd5c221"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 2s 74ms/step - loss: 1.0861 - accuracy: 0.6170\n","Test set\n"," Loss: 1.086\n","  Test Accuracy: 0.617\n","Precision is:  0.6032164042042557\n","Recall is:  0.61698956780924\n","F1 Score is:  0.6069937080316189\n","Matthew Corr Score is:  0.26197648657716915\n"]}]},{"cell_type":"markdown","metadata":{"id":"IfKwKr3CXdRe"},"source":["# Lemmatized, Synonym, Augmented Data\n"]},{"cell_type":"markdown","metadata":{"id":"Tyzvs3cnJ-XV"},"source":["# **This section shows the results for the synonym replacement dataset trained using default embeddings (for your reference to compare with back-translation results).**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDYnUbuOXhUY","outputId":"3962e65b-a585-4f8c-853f-a83b53c65e33"},"source":["train_df_syn = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/lemmatized_synonym_augment_train.csv\")\n","test_df_lem = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/test.csv\")\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/validation.csv\")\n","\n","train_df_syn = train_df_syn[['lemmatized and stopwords_removed', 'sentiment']]\n","train_df_syn = train_df_syn.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","train_df_syn = train_df_syn.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","test_df_lem = test_df_lem[['lemmatized and stopwords_removed', 'sentiment']]\n","test_df_lem = test_df_lem.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","test_df_lem = test_df_lem.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","valid_df = valid_df[['lemmatized and stopwords_removed', 'sentiment']]\n","valid_df = valid_df.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","valid_df = valid_df.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","train_df_syn['text'] = train_df_syn['text'].astype(\"str\")\n","train_df_syn['sentiment'] = train_df_syn['sentiment'].astype(\"str\")\n","\n","# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 6000\n","\n","# Max number of words in each thread. For this one, we just used the average length of word of all sentences in the data,\n","#  to prevent overfitting to the longest sentence, as most of the words are around that length. This particular var, affects the training process ALOT.\n","\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in train_df_syn['text']])\n","\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","\n","# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df_syn['text'].values)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","X_train = tokenizer.texts_to_sequences(train_df_syn['text'].values)\n","X_test = tokenizer.texts_to_sequences(test_df_lem['text'].values)\n","X_valid = tokenizer.texts_to_sequences(valid_df['text'].values)\n","\n","\n","X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n","X_valid = pad_sequences(X_valid, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Shape of data tensor (X_train):', X_train.shape)\n","print('Shape of data tensor (X_test):', X_test.shape)\n","print('Shape of data tensor (X_valid):', X_valid.shape)\n","\n","Y_train = pd.get_dummies(train_df_syn['sentiment']).values\n","Y_test = pd.get_dummies(test_df_lem['sentiment']).values\n","Y_valid = pd.get_dummies(valid_df['sentiment']).values\n","\n","print('Shape of label tensor (Y_train):', Y_train.shape)\n","print('Shape of label tensor (Y_test):', Y_test.shape)\n","\n","# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_valid.shape, Y_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 7454 unique tokens.\n","Shape of data tensor (X_train): (8162, 25)\n","Shape of data tensor (X_test): (671, 25)\n","Shape of data tensor (X_valid): (670, 25)\n","Shape of label tensor (Y_train): (8162, 3)\n","Shape of label tensor (Y_test): (671, 3)\n","(8162, 25) (8162, 3)\n","(670, 25) (670, 3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYTPr88EX4Km","outputId":"e184f67a-75b8-48ff-8706-477d7c80256d"},"source":["# LSTM\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(layers.Embedding(len(word_index)+1, EMBEDDING_DIM, input_length=X_train.shape[1]))\n","model.add(LSTM(128, return_sequences=True, dropout=0.5, recurrent_dropout=0.2))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","\n","model.summary()\n","\n","epochs = 25\n","batch_size = 16\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (None, 25, 100)           745500    \n","_________________________________________________________________\n","lstm_10 (LSTM)               (None, 25, 128)           117248    \n","_________________________________________________________________\n","lstm_11 (LSTM)               (None, 32)                20608     \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 32)                1056      \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 884,511\n","Trainable params: 884,511\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 117s 219ms/step - loss: 0.9484 - accuracy: 0.5290 - val_loss: 0.8030 - val_accuracy: 0.6567\n","Epoch 2/25\n","511/511 [==============================] - 112s 219ms/step - loss: 0.5851 - accuracy: 0.7627 - val_loss: 0.8578 - val_accuracy: 0.6627\n","Epoch 3/25\n","511/511 [==============================] - 112s 220ms/step - loss: 0.3903 - accuracy: 0.8547 - val_loss: 0.9339 - val_accuracy: 0.6343\n","Epoch 4/25\n","511/511 [==============================] - 114s 224ms/step - loss: 0.2904 - accuracy: 0.8929 - val_loss: 1.0020 - val_accuracy: 0.6791\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KX6Q7ElyaG3C","outputId":"32afa2aa-5dcd-4313-f157-ae37dd0c709e"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_lem['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_lem['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 21ms/step - loss: 1.0193 - accuracy: 0.6662\n","Test set\n"," Loss: 1.019\n","  Test Accuracy: 0.666\n","Precision is:  0.6570211455312583\n","Recall is:  0.6661698956780924\n","F1 Score is:  0.6596717180857491\n","Matthew Corr Score is:  0.35635645703901353\n"]}]},{"cell_type":"markdown","metadata":{"id":"-i6Ib--PKLAk"},"source":["# **This section will show the results for the different embeddings, for the back-translation dataset.**"]},{"cell_type":"markdown","metadata":{"id":"NTvhTjYq_5Py"},"source":["***Modelling with back-translation dataset***\n","\n","<ul>\n","  <li> LSTM with default Embeddings (results are in the first section above) </li>\n","  <li> LSTM with Word2Vec Embeddings </li>\n","  <li> LSTM with Pre-trained Word2Vec Embeddings </li>\n","  <li> LSTM with Glove Embeddings </li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"QyJjmSS0h9Oj"},"source":["# ***CBOW + LSTM (Highest Performance from above)***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukBrDCNCDRNK","outputId":"29f7dcd2-212f-460c-dec0-c40a66e0c932"},"source":["\"\"\"\n","CBOW Model\n","\"\"\"\n","text_sentences = train_df_bt['text'].apply(lambda x: x.split())\n","\n","model = gensim.models.Word2Vec(sentences=text_sentences, size=100, window=5, workers=4, min_count=1)\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","filename = 'selfdriving_embedding_word2vec.txt'\n","model.wv.save_word2vec_format(filename, binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6700\n"]}]},{"cell_type":"code","metadata":{"id":"un6CH7CdDClK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5e6acc4-ba07-4168-831b-345cf88bfc04"},"source":["import numpy as np\n","embeddings_index = {}\n","f = open(\"/content/selfdriving_embedding_word2vec.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","word2vec_embedding_matrix = np.zeros((num_words, 100))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    word2vec_embedding_matrix[i] = embedding_vector \n","print(num_words)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6692\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y51Jl4_zEGzZ","outputId":"fd8bab7a-bdc4-42da-c1ac-a020f7ce5ae0"},"source":["# CBOW LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 100, weights=[word2vec_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(256, return_sequences=True, dropout=0.5))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 16\n","\n","model.summary()\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 43, 100)           669200    \n","                                                                 \n"," lstm (LSTM)                 (None, 43, 256)           365568    \n","                                                                 \n"," lstm_1 (LSTM)               (None, 32)                36992     \n","                                                                 \n"," dense (Dense)               (None, 32)                1056      \n","                                                                 \n"," dense_1 (Dense)             (None, 3)                 99        \n","                                                                 \n","=================================================================\n","Total params: 1,072,915\n","Trainable params: 403,715\n","Non-trainable params: 669,200\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 23s 32ms/step - loss: 1.0789 - accuracy: 0.3934 - val_loss: 0.9919 - val_accuracy: 0.5672\n","Epoch 2/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0718 - accuracy: 0.4182 - val_loss: 0.9911 - val_accuracy: 0.4582\n","Epoch 3/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0663 - accuracy: 0.4202 - val_loss: 0.9892 - val_accuracy: 0.4522\n","Epoch 4/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0602 - accuracy: 0.4294 - val_loss: 1.0151 - val_accuracy: 0.3881\n","Epoch 5/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0582 - accuracy: 0.4349 - val_loss: 1.0004 - val_accuracy: 0.5209\n","Epoch 6/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0545 - accuracy: 0.4337 - val_loss: 0.9503 - val_accuracy: 0.5478\n","Epoch 7/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0514 - accuracy: 0.4414 - val_loss: 1.0005 - val_accuracy: 0.5313\n","Epoch 8/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0451 - accuracy: 0.4477 - val_loss: 0.9903 - val_accuracy: 0.4970\n","Epoch 9/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0386 - accuracy: 0.4500 - val_loss: 1.0317 - val_accuracy: 0.3925\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKaspBopkWYo","outputId":"58e65ef5-2859-48f4-83d5-8eaa03fc87f9"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_bt['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_bt['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 13ms/step - loss: 1.0448 - accuracy: 0.3651\n","Test set\n"," Loss: 1.045\n","  Test Accuracy: 0.365\n","Precision is:  0.4877826484956897\n","Recall is:  0.3651266766020864\n","F1 Score is:  0.32023744838627877\n","Matthew Corr Score is:  0.027181000001429895\n"]}]},{"cell_type":"markdown","metadata":{"id":"0c6K-zJbh48e"},"source":["# ***Skipgram Word2Vec LSTM***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L44iW96eEG12","outputId":"d86048ac-bc74-4caa-da77-9dac540188c7"},"source":["\"\"\"\n","Skip-Gram Model\n","\"\"\"\n","\n","text_sentences = train_df_bt['text'].apply(lambda x: x.split())\n","\n","model = gensim.models.Word2Vec(sentences=text_sentences, size=100, window=5, workers=4, min_count=1, sg=1)\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","filename = 'selfdriving_embedding_word2vec_skipgram.txt'\n","model.wv.save_word2vec_format(filename, binary=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6700\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkEm-eHCEG4D","outputId":"83b32cae-0a2c-49d4-b5c6-1ce80ddf2fd5"},"source":["# SkipGram word2vec\n","import numpy as np\n","embeddings_index = {}\n","f = open(\"/content/selfdriving_embedding_word2vec_skipgram.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","word2vec_embedding_matrix = np.zeros((num_words, 100))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    word2vec_embedding_matrix[i] = embedding_vector \n","print(num_words)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6692\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SGNEvlicEG5z","outputId":"32cdf62f-96a4-4c16-b1c5-60146c7442d8"},"source":["# Skipgram LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 100, weights=[word2vec_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(256, return_sequences=True, dropout=0.5))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 16\n","\n","model.summary()\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 43, 100)           669200    \n","                                                                 \n"," lstm_2 (LSTM)               (None, 43, 256)           365568    \n","                                                                 \n"," lstm_3 (LSTM)               (None, 32)                36992     \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                1056      \n","                                                                 \n"," dense_3 (Dense)             (None, 3)                 99        \n","                                                                 \n","=================================================================\n","Total params: 1,072,915\n","Trainable params: 403,715\n","Non-trainable params: 669,200\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 20s 32ms/step - loss: 1.0784 - accuracy: 0.3943 - val_loss: 1.0077 - val_accuracy: 0.4463\n","Epoch 2/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0769 - accuracy: 0.4044 - val_loss: 0.9874 - val_accuracy: 0.6254\n","Epoch 3/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0678 - accuracy: 0.4221 - val_loss: 0.9936 - val_accuracy: 0.4104\n","Epoch 4/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0530 - accuracy: 0.4484 - val_loss: 1.0045 - val_accuracy: 0.4493\n","Epoch 5/25\n","511/511 [==============================] - 15s 30ms/step - loss: 1.0466 - accuracy: 0.4462 - val_loss: 1.0259 - val_accuracy: 0.5164\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3c2tO_kXEG8V","outputId":"cdb49a25-bb1e-4f0c-fcaf-e61742d0f44b"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_bt['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_bt['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 13ms/step - loss: 1.0439 - accuracy: 0.4918\n","Test set\n"," Loss: 1.044\n","  Test Accuracy: 0.492\n","Precision is:  0.4998327680422084\n","Recall is:  0.4918032786885246\n","F1 Score is:  0.4787298883537848\n","Matthew Corr Score is:  0.10344206873497415\n"]}]},{"cell_type":"markdown","metadata":{"id":"qDejR6wWh1yG"},"source":["# ***Glove + LSTM***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HohnZV_AMkr1","outputId":"2ecd251f-d514-47a7-eeb2-ce0ed76cae3a"},"source":["# Glove\n","import numpy as np\n","embeddings_index = {}\n","f = open(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/glove.6B.300d.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, 300))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector \n","print(num_words)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6692\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6JJBVP0Mkwo","outputId":"e85bc059-3f61-46bc-de63-134609c8c5bb"},"source":["# Glove LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(256, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","model.summary()\n","\n","epochs = 25\n","batch_size = 16\n","\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 43, 300)           2007600   \n","                                                                 \n"," lstm_4 (LSTM)               (None, 43, 256)           570368    \n","                                                                 \n"," lstm_5 (LSTM)               (None, 32)                36992     \n","                                                                 \n"," dense_4 (Dense)             (None, 32)                1056      \n","                                                                 \n"," dense_5 (Dense)             (None, 3)                 99        \n","                                                                 \n","=================================================================\n","Total params: 2,616,115\n","Trainable params: 608,515\n","Non-trainable params: 2,007,600\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 22s 34ms/step - loss: 1.2811 - accuracy: 0.5011 - val_loss: 1.0980 - val_accuracy: 0.5254\n","Epoch 2/25\n","511/511 [==============================] - 16s 32ms/step - loss: 1.0474 - accuracy: 0.5562 - val_loss: 0.8985 - val_accuracy: 0.6552\n","Epoch 3/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9794 - accuracy: 0.5817 - val_loss: 0.8899 - val_accuracy: 0.6328\n","Epoch 4/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9563 - accuracy: 0.5896 - val_loss: 0.9620 - val_accuracy: 0.5806\n","Epoch 5/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9363 - accuracy: 0.6018 - val_loss: 0.8765 - val_accuracy: 0.6507\n","Epoch 6/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9310 - accuracy: 0.6089 - val_loss: 0.8526 - val_accuracy: 0.6403\n","Epoch 7/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9150 - accuracy: 0.6213 - val_loss: 0.8750 - val_accuracy: 0.6418\n","Epoch 8/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9177 - accuracy: 0.6219 - val_loss: 0.9030 - val_accuracy: 0.6209\n","Epoch 9/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9008 - accuracy: 0.6378 - val_loss: 0.8872 - val_accuracy: 0.6657\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liv8G6GrEG-i","outputId":"625a3bd5-190e-46d4-fd04-3883d70cee48"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_bt['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_bt['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 14ms/step - loss: 0.8758 - accuracy: 0.6602\n","Test set\n"," Loss: 0.876\n","  Test Accuracy: 0.660\n","Precision is:  0.6707380210997993\n","Recall is:  0.6602086438152012\n","F1 Score is:  0.6647920304655108\n","Matthew Corr Score is:  0.38326362998216223\n"]}]},{"cell_type":"markdown","metadata":{"id":"cnKLeM2Ghte7"},"source":["# ***Word2Vec Pretrained w/ LSTM***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3JHIPBOe375","outputId":"8c8a198a-4dc1-4dcd-9b35-b7c5a43018d0"},"source":["train_df_bt = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/lemmatized_bt_augment_train.csv\")\n","test_df_bt = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/test.csv\")\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IS460 ML Datasets/latest/validation.csv\")\n","\n","train_df_bt = train_df_bt[['lemmatized and stopwords_removed', 'sentiment']]\n","train_df_bt = train_df_bt.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","train_df_bt = train_df_bt.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","test_df_bt = test_df_bt[['lemmatized and stopwords_removed', 'sentiment']]\n","test_df_bt = test_df_bt.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","test_df_bt = test_df_bt.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","valid_df = valid_df[['lemmatized and stopwords_removed', 'sentiment']]\n","valid_df = valid_df.rename(columns={\"lemmatized and stopwords_removed\": \"text_cleaned\"})\n","valid_df = valid_df.rename(columns={\"text_cleaned\": \"text\"}) \n","\n","train_df_bt['text'] = train_df_bt['text'].astype(\"str\")\n","train_df_bt['sentiment'] = train_df_bt['sentiment'].astype(\"str\")\n","\n","# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 6000\n","\n","# Max number of words in each thread. For this one, we just used the average length of word of all sentences in the data,\n","#  to prevent overfitting to the longest sentence, as most of the words are around that length. This particular var, affects the training process ALOT.\n","\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in train_df_bt['text']])\n","\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","\n","# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df_bt['text'].values)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","X_train = tokenizer.texts_to_sequences(train_df_bt['text'].values)\n","X_test = tokenizer.texts_to_sequences(test_df_bt['text'].values)\n","X_valid = tokenizer.texts_to_sequences(valid_df['text'].values)\n","\n","\n","X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n","X_valid = pad_sequences(X_valid, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Shape of data tensor (X_train):', X_train.shape)\n","print('Shape of data tensor (X_test):', X_test.shape)\n","print('Shape of data tensor (X_valid):', X_valid.shape)\n","\n","Y_train = pd.get_dummies(train_df_bt['sentiment']).values\n","Y_test = pd.get_dummies(test_df_bt['sentiment']).values\n","Y_valid = pd.get_dummies(valid_df['sentiment']).values\n","\n","print('Shape of label tensor (Y_train):', Y_train.shape)\n","print('Shape of label tensor (Y_test):', Y_test.shape)\n","\n","# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_valid.shape, Y_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6691 unique tokens.\n","Shape of data tensor (X_train): (8162, 43)\n","Shape of data tensor (X_test): (671, 43)\n","Shape of data tensor (X_valid): (670, 43)\n","Shape of label tensor (Y_train): (8162, 3)\n","Shape of label tensor (Y_test): (671, 3)\n","(8162, 43) (8162, 3)\n","(670, 43) (670, 3)\n"]}]},{"cell_type":"code","metadata":{"id":"t9OwXZbXQeAo"},"source":["import gensim.downloader as api\n","model = api.load(\"word2vec-google-news-300\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCfeydCTPW_r","outputId":"9c2c7385-3a3f-449e-f9fb-3af63cf71213"},"source":["\"\"\"\n","Pre-trained Word2Vec Model\n","\"\"\"\n","\n","text_sentences = train_df_bt['text'].apply(lambda x: x.split())\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","filename = 'selfdriving_embedding_word2vec_pretrained.txt'\n","model.wv.save_word2vec_format(filename, binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  # Remove the CWD from sys.path while we load stuff.\n"]},{"output_type":"stream","name":"stdout","text":["3000000\n"]}]},{"cell_type":"code","metadata":{"id":"ZuawQqNGdppK"},"source":["import numpy as np\n","labels = np.asarray(model.index2word)\n","vectors = np.asarray(model.vectors)\n","word_embeddings = dict(zip(labels, vectors))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1sLEiozd03v","outputId":"83b34aff-6111-4acd-de35-50b4444c9fdb"},"source":["num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, 300))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = word_embeddings.get(word) \n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector \n","print(num_words)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6692\n"]}]},{"cell_type":"code","metadata":{"id":"NFhIqkMEPXER","colab":{"base_uri":"https://localhost:8080/"},"outputId":"55298cd6-24e0-4a48-9658-156fe3b9ce91"},"source":["# Pre-trained Word2Vec LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(256, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(LSTM(32))\n","model.add(Dense(32,activation='relu')) \n","model.add(layers.Dense(3, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"categorical_crossentropy\", \n","     metrics=['accuracy'])\n","\n","model.summary() \n","\n","epochs = 25\n","batch_size = 16\n","\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), callbacks=[es])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 43, 300)           2007600   \n","                                                                 \n"," lstm (LSTM)                 (None, 43, 256)           570368    \n","                                                                 \n"," lstm_1 (LSTM)               (None, 32)                36992     \n","                                                                 \n"," dense (Dense)               (None, 32)                1056      \n","                                                                 \n"," dense_1 (Dense)             (None, 3)                 99        \n","                                                                 \n","=================================================================\n","Total params: 2,616,115\n","Trainable params: 608,515\n","Non-trainable params: 2,007,600\n","_________________________________________________________________\n","Epoch 1/25\n","511/511 [==============================] - 25s 34ms/step - loss: 1.2627 - accuracy: 0.5070 - val_loss: 1.0986 - val_accuracy: 0.5328\n","Epoch 2/25\n","511/511 [==============================] - 16s 32ms/step - loss: 1.0452 - accuracy: 0.5511 - val_loss: 0.8626 - val_accuracy: 0.6642\n","Epoch 3/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9772 - accuracy: 0.5718 - val_loss: 0.8933 - val_accuracy: 0.6358\n","Epoch 4/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9546 - accuracy: 0.5848 - val_loss: 0.9726 - val_accuracy: 0.5896\n","Epoch 5/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9379 - accuracy: 0.5952 - val_loss: 0.8407 - val_accuracy: 0.6642\n","Epoch 6/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9301 - accuracy: 0.6001 - val_loss: 0.8454 - val_accuracy: 0.6597\n","Epoch 7/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9209 - accuracy: 0.6087 - val_loss: 0.8207 - val_accuracy: 0.6776\n","Epoch 8/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9181 - accuracy: 0.6131 - val_loss: 0.8003 - val_accuracy: 0.6955\n","Epoch 9/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9101 - accuracy: 0.6125 - val_loss: 0.8806 - val_accuracy: 0.6552\n","Epoch 10/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9023 - accuracy: 0.6204 - val_loss: 0.8232 - val_accuracy: 0.6881\n","Epoch 11/25\n","511/511 [==============================] - 16s 32ms/step - loss: 0.9069 - accuracy: 0.6198 - val_loss: 0.8520 - val_accuracy: 0.6806\n"]}]},{"cell_type":"code","metadata":{"id":"HNMx6-OUPXGw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cbc12623-e71b-4c16-bb87-7b35c79ecf3c"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n Loss: {:0.3f}\\n  Test Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = convert_prediction_1D(predictions)\n","\n","\n","precision, recall, f1_score, none = precision_recall_fscore_support(test_df_bt['sentiment'], predictions, average='weighted')\n","m_corr = matthews_corrcoef(test_df_bt['sentiment'], predictions)\n","\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)\n","print(\"Matthew Corr Score is: \", m_corr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21/21 [==============================] - 0s 14ms/step - loss: 0.8713 - accuracy: 0.6453\n","Test set\n"," Loss: 0.871\n","  Test Accuracy: 0.645\n","Precision is:  0.6679277370159373\n","Recall is:  0.6453055141579732\n","F1 Score is:  0.6535235493623104\n","Matthew Corr Score is:  0.3754780956171252\n"]}]},{"cell_type":"code","metadata":{"id":"tAHW9v-pznO9"},"source":[""],"execution_count":null,"outputs":[]}]}